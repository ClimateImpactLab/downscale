# Rolling QDM test case
apiVersion: argoproj.io/v1alpha1
kind: Workflow
metadata:
  generateName: dc6-dev-
  labels:
    env: dev
spec:
  arguments:
    parameters:
    - name: reference-zarr
      value: "scratch/clean-dev/ERA-5/tasmax.1995-2014.0p25.zarr"
    - name: gcm-historical-zarr
      value: "scratch/clean-cmip6-dev-2phms/historical-sliced.zarr"
    - name: gcm-future-zarr
      value: "scratch/clean-cmip6-dev-2phms/future-standardized.zarr"
    - name: out-zarr
      value: "scratch/{{ workflow.name }}/biascorrected.zarr"
    - name: target-variable
      value: tasmax
    - name: halfwindow-length
      value: 10
    - name: n-quantiles
      value: 100

  serviceAccountName: workflows-default

    # So workers go on larger, spot, worker node pool.
  nodeSelector:
    dedicated: worker
  tolerations:
  - key: dedicated
    operator: "Equal"
    value: "worker"
    effect: "NoSchedule"
  - key: kubernetes.azure.com/scalesetpriority
    operator: "Equal"
    value: "spot"
    effect: "NoSchedule"

  entrypoint: main
  templates:


  - name: main
    dag:
      tasks:
      - name: reference-regrid
        template: regrid
        arguments:
          parameters:
          - name: in-zarr
            value: "{{ workflow.parameters.reference-zarr }}"
          - name: out-zarr
            value: "scratch/{{ workflow.name }}/reference-regrid.zarr"
          - name: regrid-method
            value: "bilinear"
          - name: target-resolution
            value: "1.0"
      - name: reference-rechunk
        dependencies: [reference-regrid]
        template: rechunk
        arguments:
          parameters:
          - name: in-zarr
            value: "scratch/{{ workflow.name }}/reference-regrid.zarr"
          - name: out-zarr
            value: "scratch/{{ workflow.name }}/reference-rechunk.zarr"
          - name: time-chunk
            value: "-1"
          - name: lat-chunk
            value: 10
          - name: lon-chunk
            value: 10
      - name: gcm-historical-regrid
        template: regrid
        arguments:
          parameters:
          - name: in-zarr
            value: "{{ workflow.parameters.gcm-historical-zarr }}"
          - name: out-zarr
            value: "scratch/{{ workflow.name }}/gcm-historical-regrid.zarr"
          - name: regrid-method
            value: "bilinear"
          - name: target-resolution
            value: "1.0"
      - name: gcm-historical-rechunk
        dependencies: [gcm-historical-regrid]
        template: rechunk
        arguments:
          parameters:
          - name: in-zarr
            value: "scratch/{{ workflow.name }}/gcm-historical-regrid.zarr"
          - name: out-zarr
            value: "scratch/{{ workflow.name }}/gcm-historical-rechunk.zarr"
          - name: time-chunk
            value: "-1"
          - name: lat-chunk
            value: 10
          - name: lon-chunk
            value: 10
      - name: gcm-future-regrid
        template: regrid
        arguments:
          parameters:
          - name: in-zarr
            value: "{{ workflow.parameters.gcm-future-zarr }}"
          - name: out-zarr
            value: "scratch/{{ workflow.name }}/gcm-future-regrid.zarr"
          - name: regrid-method
            value: "bilinear"
          - name: target-resolution
            value: "1.0"
      - name: gcm-future-rechunk
        dependencies: [gcm-future-regrid]
        template: rechunk
        arguments:
          parameters:
          - name: in-zarr
            value: "scratch/{{ workflow.name }}/gcm-future-regrid.zarr"
          - name: out-zarr
            value: "scratch/{{ workflow.name }}/gcm-future-rechunk.zarr"
          - name: time-chunk
            value: "-1"
          - name: lat-chunk
            value: 10
          - name: lon-chunk
            value: 10
      - name: biascorrect
        dependencies: [gcm-historical-rechunk, gcm-future-rechunk, reference-rechunk]
        template: biascorrect-qdm
        arguments:
          parameters:  # TODO: I STOPPED HERE. need to fill in args below, rename above parameters, inputs to match below nomenclature.
            - name: variable
              value: "{{ workflow.parameters.target-variable }}"
            - name: ref-zarr
              value: "scratch/{{ workflow.name }}/reference-rechunk.zarr"
            - name: hist-zarr
              value: "scratch/{{ workflow.name }}/gcm-historical-rechunk.zarr"
            - name: future-zarr
              value: "scratch/{{ workflow.name }}/gcm-future-rechunk.zarr"
            - name: out-zarr
              value: "{{ workflow.parameters.out-zarr }}"
            - name: n-quantiles
              value: "{{ workflow.parameters.n-quantiles }}"
            - name: halfwindow-length
              value: "{{ workflow.parameters.halfwindow-length }}"


  - name: regrid-dodola
    inputs:
      parameters:
      - name: in-zarr
      - name: out-zarr
      - name: regrid-method
      - name: target-resolution
    container:
      image: downscalecmip6.azurecr.io/dodola:0.1.0
      env:
      - name: AZURE_STORAGE_ACCOUNT
        valueFrom:
          secretKeyRef:
            name: workerstoragecreds-secret
            key: azurestorageaccount
      - name: AZURE_STORAGE_KEY
        valueFrom:
          secretKeyRef:
            name: workerstoragecreds-secret
            key: azurestoragekey
      command: ["dodola"]
      args:
      - "regrid" 
      - "{{ inputs.parameters.in-zarr }}"
      - "--out" 
      - "{{ inputs.parameters.out-zarr }}"
      - "--method"
      - "{{ inputs.parameters.regrid-method }}" 
      - "--targetresolution"
      - "{{ inputs.parameters.target-resolution }}"
      resources:
        requests:
          memory: 8Gi
          cpu: "1000m"
        limits:
          memory: 8Gi
          cpu: "2000m"
    activeDeadlineSeconds: 900
    retryStrategy:
      limit: 4
      retryPolicy: "Always"


  - name: regrid
    # This is a work around until https://github.com/ClimateImpactLab/dodola/issues/38 is cleared.
    inputs:
      parameters:
      - name: in-zarr
      - name: out-zarr
      - name: regrid-method
      - name: target-resolution
    outputs:
      parameters:
      - name: out-zarr
        value: "{{ inputs.parameters.out-zarr }}"
    script:
      image: downscalecmip6.azurecr.io/dodola:0.1.0
      env:
      - name: IN_ZARR
        value: "{{ inputs.parameters.in-zarr }}"
      - name: OUT_ZARR
        value: "{{ inputs.parameters.out-zarr }}"
      - name: REGRID_METHOD
        value: "{{ inputs.parameters.regrid-method }}"
      - name: TARGET_RESOLUTION
        value: "{{ inputs.parameters.target-resolution }}"
      - name: AZURE_STORAGE_ACCOUNT_NAME
        valueFrom:
          secretKeyRef:
            name: workerstoragecreds-secret
            key: azurestorageaccount
      - name: AZURE_STORAGE_ACCOUNT_KEY
        valueFrom:
          secretKeyRef:
            name: workerstoragecreds-secret
            key: azurestoragekey
      command: [python]
      source: |
        import os
        import xesmf as xe
        import numpy as np
        import xarray as xr
        from adlfs import AzureBlobFileSystem
        
        fs = AzureBlobFileSystem()
        in_ds = xr.open_zarr(fs.get_mapper(os.environ.get("IN_ZARR")), chunks="auto")
        target_resolution = np.float(os.environ.get("TARGET_RESOLUTION"))
        target = xe.util.grid_global(target_resolution, target_resolution)
        weights_path = None

        # Convert 2D -> 1D dimensions in target coordinates so
        # output coordinate dimensions are 1D.
        target["lat"] = np.unique(target["lat"].values)
        target["lon"] = np.unique(target["lon"].values)

        regridder = xe.Regridder(
            in_ds,
            target,
            method=os.environ.get("REGRID_METHOD", None),
            filename=weights_path,
        )
        out = regridder(in_ds)

        out.to_zarr(
            fs.get_mapper(os.environ.get("OUT_ZARR")),
            mode="w",
            compute=True
        )
      resources:
        requests:
          memory: 48Gi
          cpu: "1000m"
        limits:
          memory: 48Gi
          cpu: "2000m"
    activeDeadlineSeconds: 1800
    retryStrategy:
      limit: 2
      retryPolicy: "Always"


  - name: rechunk
    inputs:
      parameters:
      - name: in-zarr
      - name: out-zarr
      - name: time-chunk
      - name: lat-chunk
      - name: lon-chunk
    outputs:
      parameters:
        - name: out-zarr
          value: "{{ inputs.parameters.out-zarr }}"
    container:
      image: downscalecmip6.azurecr.io/dodola:dev
      imagePullPolicy: Always
      env:
      - name: AZURE_STORAGE_ACCOUNT_NAME
        valueFrom:
          secretKeyRef:
            name: workerstoragecreds-secret
            key: azurestorageaccount
      - name: AZURE_STORAGE_ACCOUNT_KEY
        valueFrom:
          secretKeyRef:
            name: workerstoragecreds-secret
            key: azurestoragekey
      command: [dodola]
      args: 
      - "rechunk"
      - "az://{{ inputs.parameters.in-zarr }}"
      - "--out"
      - "az://{{ inputs.parameters.out-zarr }}"
      - "--chunk"
      - "time={{ inputs.parameters.time-chunk }}"
      - "--chunk"
      - "lat={{ inputs.parameters.lat-chunk }}"
      - "--chunk" 
      - "lon={{ inputs.parameters.lon-chunk }}"
      resources:
        requests:
          memory: 24Gi
          cpu: "1000m"
        limits:
          memory: 24Gi
          cpu: "2000m"
    activeDeadlineSeconds: 10800


  - name: biascorrect-qdm
    inputs:
      parameters:
      - name: variable
      - name: ref-zarr
      - name: hist-zarr
      - name: future-zarr
      - name: out-zarr
      - name: n-quantiles
      - name: halfwindow-length
    outputs:
      parameters:
      - name: out-zarr
        valueFrom:
          parameter: "{{ tasks.netcdfs2zarr.outputs.parameters.out-zarr }}"
    dag:
      tasks:
      - name: train-qdm
        template: train-qdm
        arguments:
          parameters:
          - name: variable
            value: "{{ inputs.parameters.variable }}"
          - name: ref-zarr
            value: "{{ inputs.parameters.ref-zarr }}"
          - name: hist-zarr
            value: "{{ inputs.parameters.hist-zarr }}"
          - name: out-zarr
            value: "scratch/{{workflow.name}}/qdm-model.zarr"
          - name: n-quantiles
            value: "{{ inputs.parameters.n-quantiles }}"
      - name: get-rollingyearwindow-range
        template: get-rollingyearwindow-range
        arguments:
          parameters:
          - name: variable
            value: "{{ inputs.parameters.variable }}"
          - name: future-zarr
            value: "{{ inputs.parameters.future-zarr }}"
          - name: halfwindow-length
            value: "{{ inputs.parameters.halfwindow-length }}"
      - name: qdm-adjust-year
        dependencies: [get-rollingyearwindow-range, train-qdm]
        template: qdm-adjust-year
        arguments:
          parameters:
          - name: future-zarr
            value: "{{ inputs.parameters.future-zarr }}"
          - name: out
            value: "scratch/{{workflow.name}}/qdm-years/{{item}}.nc"
          - name: variable
            value: "{{ inputs.parameters.variable }}"
          - name: year
            value: "{{item}}"
          - name: qdm-model-zarr
            value: "scratch/{{workflow.name}}/qdm-model.zarr"
          - name: halfwindow-length
            value: "{{ inputs.parameters.halfwindow-length }}"
        withSequence:
          start: "{{ tasks.get-rollingyearwindow-range.outputs.parameters.firstyear }}"
          end: "{{ tasks.get-rollingyearwindow-range.outputs.parameters.lastyear }}"
      - name: netcdfs2zarr
        dependencies: [qdm-adjust-year]
        template: netcdfs2zarr
        arguments:
          parameters: 
          - name: in-dir
            value: "scratch/{{workflow.name}}/qdm-years/"
          - name: out-zarr
            value: "{{ inputs.parameters.out-zarr }}"


  - name: get-rollingyearwindow-range
    # Extracts the first and last year we can use for rolling-window QDM from input file time dim.
    inputs:
      parameters:
      - name: future-zarr
      - name: halfwindow-length
        value: 10
    outputs:
      parameters:
      - name: firstyear
        valueFrom:
          path: /mnt/out/firstyear.txt
      - name: lastyear
        valueFrom:
          path: /mnt/out/lastyear.txt
    script:
      image: downscalecmip6.azurecr.io/dodola:0.1.0
      env:
      - name: FUTURE_ZARR
        value: "{{ inputs.parameters.future-zarr }}"
      - name: HALFWINDOW_LENGTH
        value: "{{ inputs.parameters.halfwindow-length }}"
      - name: AZURE_STORAGE_ACCOUNT_NAME
        valueFrom:
          secretKeyRef:
            name: workerstoragecreds-secret
            key: azurestorageaccount
      - name: AZURE_STORAGE_ACCOUNT_KEY
        valueFrom:
          secretKeyRef:
            name: workerstoragecreds-secret
            key: azurestoragekey
      command: [python]
      source: |
        import os
        import numpy as np
        import xarray as xr
        from adlfs import AzureBlobFileSystem
        
        n = int(os.environ.get("HALFWINDOW_LENGTH"))
        fs = AzureBlobFileSystem()
        d = xr.open_zarr(fs.get_mapper(os.environ.get("FUTURE_ZARR")))

        # +1 yr because we need 15 days from end of the first year:
        firstyear = d["time"].dt.year.values[0] + n + 1
        # -2 yrs broken down: -1 because argo has inclusive ranges & -1 because we need 15 days from final year:
        lastyear = d["time"].dt.year.values[-1] - n - 2
        print(f"output 'firstyear': {firstyear}")
        print(f"output 'lastyear': {lastyear}")

        if firstyear > lastyear:
          raise ValueError("firstyear must be <= lastyear to have years for QDM window.")

        # Safety against spending lots and lots of time and money:
        assert abs(firstyear - lastyear) < 200, "dif between firstyear and lastyear seems too large, error?" 

        with open("/mnt/out/firstyear.txt", "w") as fl:
          fl.write(str(firstyear))
        with open("/mnt/out/lastyear.txt", "w") as fl:
          fl.write(str(lastyear))
      resources:
        requests:
          memory: 250Mi
          cpu: "1000m"
        limits:
          memory: 500Mi
          cpu: "1000m"
# emptyDir volume as k8sapi can't output to base image layer:
      volumeMounts:
      - name: out
        mountPath: /mnt/out
    volumes:
      - name: out
        emptyDir: { }
    activeDeadlineSeconds: 300
    retryStrategy:
      limit: 4
      retryPolicy: "Always"


  - name: train-qdm
    inputs:
      parameters:
      - name: variable
      - name: ref-zarr
      - name: hist-zarr
      - name: out-zarr
      - name: n-quantiles
        value: 100
    outputs:
      parameters:
      - name: out-zarr
        value: "{{ inputs.parameters.out-zarr }}"
    script:
      image: downscalecmip6.azurecr.io/dodola:0.1.0
      env:
      - name: VARIABLE
        value: "{{  inputs.parameters.variable }}"
      - name: REF_PATH
        value: "{{  inputs.parameters.ref-zarr }}"
      - name: HIST_PATH
        value: "{{  inputs.parameters.hist-zarr }}"
      - name: OUT
        value: "{{ inputs.parameters.out-zarr }}"
      - name: N_QUANTILES
        value: "{{ inputs.parameters.n-quantiles }}"
      - name: AZURE_STORAGE_ACCOUNT_NAME
        valueFrom:
          secretKeyRef:
            name: workerstoragecreds-secret
            key: azurestorageaccount
      - name: AZURE_STORAGE_ACCOUNT_KEY
        valueFrom:
          secretKeyRef:
            name: workerstoragecreds-secret
            key: azurestoragekey
      command: [python]
      source: |
        # Download required ufunc branch of Ouranosinc/xclim to avoid memory explosion.
        import subprocess
        
        subprocess.run(
            # ["python", "-m", "pip", "install", "git+https://github.com/Ouranosinc/xclim@103fbe0abfc14a2faafc5525e80a3aad90131c7f", "--upgrade"], 
            ["python", "-m", "pip", "install", "git+https://github.com/Ouranosinc/xclim@sdba-ufunc", "--upgrade"], 
            stdout=subprocess.PIPE, 
            universal_newlines=True,
            check=True
        )

        # Real workflow begins here:
        import os
        from xclim import sdba
        import numpy as np
        import xarray as xr
        from adlfs import AzureBlobFileSystem
        
        target_variable = os.environ.get("VARIABLE")
        fs = AzureBlobFileSystem()
        refdf = xr.open_zarr(fs.get_mapper(os.environ.get("REF_PATH")), chunks="auto")
        histdf = xr.open_zarr(fs.get_mapper(os.environ.get("HIST_PATH")), chunks="auto")

        QDMdg = sdba.adjustment.QuantileDeltaMapping(
            kind="+",   # <- ONLY FOR TEMPERATURE, NOT PRECIPITATION!
            group=sdba.Grouper("time.dayofyear", window=31),
            nquantiles=int(os.environ.get("N_QUANTILES"))
        )
        QDMdg.train(
            ref=refdf[target_variable],
            hist=histdf[target_variable]
        )

        print(QDMdg)  # DEBUG
        print(QDMdg._hist_calendar)  # DEBUG

        QDMdg.ds.to_zarr(
            store=fs.get_mapper(os.environ.get("OUT")),
            mode="w"
        )
      resources:
        requests:
          memory: 48Gi
          cpu: "2000m"
        limits:
          memory: 48Gi
          cpu: "2500m"
    activeDeadlineSeconds: 172800
    retryStrategy:
      limit: 4
      retryPolicy: "Always"


  - name: qdm-adjust-year
    inputs:
      parameters:
      - name: future-zarr
      - name: year
      - name: out
      - name: qdm-model-zarr
      - name: halfwindow-length
      - name: variable
    script:
      image: downscalecmip6.azurecr.io/dodola:0.1.0
      env:
      - name: SIM_PATH
        value: "{{ inputs.parameters.future-zarr }}"
      - name: QDM_MODEL_ZARR
        value: "{{ inputs.parameters.qdm-model-zarr }}"
      - name: TARGET_YEAR
        value: "{{ inputs.parameters.year }}"
      - name: VARIABLE
        value: "{{ inputs.parameters.variable }}"
      - name: OUT
        value: "{{ inputs.parameters.out }}"
      - name: HALFWINDOW_LENGTH
        value: "{{ inputs.parameters.halfwindow-length }}"
      - name: AZURE_STORAGE_ACCOUNT_NAME
        valueFrom:
          secretKeyRef:
            name: workerstoragecreds-secret
            key: azurestorageaccount
      - name: AZURE_STORAGE_ACCOUNT_KEY
        valueFrom:
          secretKeyRef:
            name: workerstoragecreds-secret
            key: azurestoragekey
      - name: PYTHONUNBUFFERED
        value: "FALSE"  # So we get instant logs
      command: [python]
      source: |
        print('Starting QDM-adjust for single year')
        # Download required ufunc branch of Ouranosinc/xclim to avoid memory explosion.
        import subprocess
        
        subprocess.run(
            # ["python", "-m", "pip", "install", "git+https://github.com/Ouranosinc/xclim@103fbe0abfc14a2faafc5525e80a3aad90131c7f", "--upgrade"], 
            ["python", "-m", "pip", "install", "git+https://github.com/Ouranosinc/xclim@sdba-ufunc", "--upgrade"], 
            stdout=subprocess.PIPE,
            universal_newlines=True,
            check=True
        )
        print("Installed custom xclim branch with pip")  # DEBUG

        # Real workflow begins here:
        import os
        from xclim import sdba
        import numpy as np
        import xarray as xr
        from adlfs import AzureBlobFileSystem
        
        roll = int(os.environ.get("HALFWINDOW_LENGTH"))
        yr = int(os.environ.get("TARGET_YEAR"))
        target_variable = os.environ.get("VARIABLE")

        fs = AzureBlobFileSystem()

        print("Loaded QDM model data")
        QDMdg = sdba.adjustment.QuantileDeltaMapping.from_dataset(
            xr.open_dataset(
                fs.get_mapper(os.environ.get("QDM_MODEL_ZARR")),
                engine="zarr",
                chunks="auto"
            )
        )
        print("Created QDM model")
        # Slice to get 15 days before and after our target year. This accounts
        # for the rolling 31 day rolling window.
        timeslice = slice(f"{yr - roll - 1}-12-17", f"{yr + roll + 1}-01-15")

        print("Setup I/O for data-to-adjust")
        simdf = xr.open_dataset(
            fs.get_mapper(os.environ.get("SIM_PATH")),
            engine="zarr",
            chunks="auto"
        )
        simdf = simdf[target_variable].sel(time=timeslice)

        y = QDMdg.adjust(simdf).sel(time=str(yr))
        print("Simulation adjusted")
        local_path = f"/mnt/out/{yr}.nc"
        y.to_netcdf(local_path)
        print("Adjusted data written to local disk")

        # Move to specific location on AZ storage.
        # Work around for wonky artifact repo movement in argo.
        fs.put(local_path, os.environ.get("OUT"))
        print("Done")
      resources:
        requests:
          memory: 42Gi
          cpu: "2000m"
        limits:
          memory: 42Gi
          cpu: "2000m"
# emptyDir volume as k8sapi can't output to base image layer:
      volumeMounts:
      - name: out
        mountPath: /mnt/out
    volumes:
      - name: out
        emptyDir: { }
    activeDeadlineSeconds: 86400
    retryStrategy:
      limit: 2
      retryPolicy: "Always"


  - name: netcdfs2zarr
    inputs:
      parameters:
      - name: in-dir
      - name: out-zarr
    outputs:
      parameters:
      - name: out-zarr
        value: "{{ inputs.parameters.out-zarr }}"
    script:
      image: downscalecmip6.azurecr.io/dodola:0.1.0
      env:
      - name: IN
        value: "{{ inputs.parameters.in-dir }}"
      - name: OUT
        value: "{{ inputs.parameters.out-zarr }}"
      - name: AZURE_STORAGE_ACCOUNT_NAME
        valueFrom:
          secretKeyRef:
            name: workerstoragecreds-secret
            key: azurestorageaccount
      - name: AZURE_STORAGE_ACCOUNT_KEY
        valueFrom:
          secretKeyRef:
            name: workerstoragecreds-secret
            key: azurestoragekey
      command: [python]
      source: |
        # Real workflow begins here:
        import os
        import numpy as np
        import xarray as xr
        from adlfs import AzureBlobFileSystem
        

        fs = AzureBlobFileSystem()
        # Grab dir of yearly NetCDFs, read into single remote Zarr.
        local_stash = "/mnt/in/"
        fs.get(rpath=os.environ.get("IN"), lpath=local_stash, recursive=True)

        d = xr.open_mfdataset(
            f"{local_stash}*.nc",
            concat_dim="time"
        )
        d.to_zarr(
            fs.get_mapper(os.environ.get("OUT")),
            mode="w",
            compute=True
        )
      resources:
        requests:
          memory: 20Gi
          cpu: "2000m"
        limits:
          memory: 20Gi
          cpu: "2000m"
# emptyDir volume as k8sapi can't output to base image layer:
      volumeMounts:
      - name: in
        mountPath: /mnt/in
    volumes:
      - name: in
        emptyDir: { }
    activeDeadlineSeconds: 480
    retryStrategy:
      limit: 4
      retryPolicy: "Always"


# Prints data on stored zarr file to log.
  - name: printzarr
    inputs:
      parameters:
      - name: in-zarr
    script:
      image: downscalecmip6.azurecr.io/dodola:0.1.0
      env:
      - name: IN_ZARR
        value: "{{ inputs.parameters.in-zarr }}"
      - name: AZURE_STORAGE_ACCOUNT_NAME
        valueFrom:
          secretKeyRef:
            name: workerstoragecreds-secret
            key: azurestorageaccount
      - name: AZURE_STORAGE_ACCOUNT_KEY
        valueFrom:
          secretKeyRef:
            name: workerstoragecreds-secret
            key: azurestoragekey
      command: [python]
      source: |
        # Real workflow begins here:
        import os
        import xarray as xr
        from adlfs import AzureBlobFileSystem
        

        fs = AzureBlobFileSystem()

        target = os.environ.get("IN_ZARR")
        print(f"opening {target}")

        d = xr.open_zarr(
            fs.get_mapper(target)
        )

        print(d)
      resources:
        requests:
          memory: 500Mi
          cpu: "1000m"
        limits:
          memory: 1Gi
          cpu: "1000m"
    activeDeadlineSeconds: 900
