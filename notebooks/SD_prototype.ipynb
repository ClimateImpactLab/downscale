{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import xarray as xr\n",
    "import collections\n",
    "import os \n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "import xesmf as xe\n",
    "\n",
    "from utils import _convert_lons, _remove_leap_days, compute_daily_climo\n",
    "from regridding import apply_weights\n",
    "\n",
    "import dask.distributed as dd\n",
    "import dask_kubernetes as dk\n",
    "import dask\n",
    "import rhg_compute_tools.kubernetes as rhgk"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### This notebook is a test of all the steps for Spatial Disaggregation to get a handle on the total CPU time it will take for this part of BCSD. \n",
    "\n",
    "Once-off steps: \n",
    "\n",
    "1. compute multi-decade daily climatologies of ERA-5 at obs-res and coarsen it to model-res (they, e.g. NASA-NEX, do not say how, we will do bilinear for consistency with later step)\n",
    "\n",
    "Per model/scenario/experiment steps:\n",
    "\n",
    "1. subtract (or divide for precip) BC’ed model data at model-res from obs climo at model resolution to calculate a “scaling factor” \n",
    "2. bilinearly interpolate “scaling factor” (using xESMF) from the model grid to the obs grid \n",
    "3. Apply scaling factor by adding (for temp) and multiplying (for precip) the “scaling factor” to the obs-res daily climatology \n",
    "\n",
    "NOTE: For the purpose of being conservative with timing, the \"coarsen obs climatology step to model-res\" is in the per model/scenario/experiment step, since we don't know for sure how/if CMIP6 models will be at exactly the same resolution. \n",
    "\n",
    "Currently this workflow is only built out for temperature, not precipitation. All steps are included, the last step (applying the interpolated scale factor to the obs-res daily climatology) has not yet been tested. All other parts of the workflow have been tested. The second to last step, the interpolation of the scaling factor from coarse to fine, is the most memory intensive, thus I have only tested for a subset of timesteps. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "611a52d857744f4a9b4d8936bd742ef9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(HTML(value='<h2>KubeCluster</h2>'), HBox(children=(HTML(value='\\n<div>\\n  <style scoped>\\n    .…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "client, cluster = rhgk.get_standard_cluster()\n",
    "cluster"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "load test bias corrected output from global bias correction prototype notebook (BC'ed NASA GISS CMIP6 data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "tmax_model = xr.open_dataset('/home/jovyan/global_bias_corrected_tenyears.nc').rename(\n",
    "                             {'lat': 'latitude', 'lon': 'longitude', \n",
    "                              '__xarray_dataarray_variable__': 'tasmax'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "tmax_obs = xr.open_mfdataset(os.path.join('/gcs/rhg-data/climate/source_data/GMFD/tmax', \n",
    "                                         'tmax_0p25_daily_199*'), concat_dim='time',\n",
    "                              parallel=True).squeeze(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# standardize longitudes \n",
    "tmax_obs = _convert_lons(tmax_obs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Remove leap days from obs "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# remove leap days \n",
    "# tmax_obs = tmax_obs.sel(time=~((tmax_obs.time.dt.month == 2) & (tmax_obs.time.dt.day == 29)))\n",
    "tmax_obs = _remove_leap_days(tmax_obs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "size_model = len(tmax_model.latitude) * len(tmax_model.longitude)\n",
    "size_obs = len(tmax_obs.latitude) * len(tmax_obs.longitude)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "space_chunks = {'time': -1, 'latitude': 75, 'longitude': 75}\n",
    "day_chunks = {'dayofyear': 1, 'latitude': -1, 'longitude': -1}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Compute obs climatology (obs res)\n",
    "\n",
    "Note: in real workflow, this will be pre-computed (only need to do this once) and loaded. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''%%time \n",
    "tmax_obs = tmax_obs.chunk(space_chunks)\n",
    "tmax_obs_lazy = compute_daily_climo(tmax_obs['tmax'])\n",
    "climo_obs_fine = tmax_obs_lazy.persist()\n",
    "# rechunk climo fine into day chunks \n",
    "climo_obs_fine = climo_obs_fine.chunk(day_chunks)\n",
    "climo_obs_fine = climo_obs_fine.compute()\n",
    "climo_attrs = {\"file description\": \"daily climatology for 1990s, without leap years, from GMFD\", \n",
    "               \"author\": \"Diana Gergel\", \"contact\": \"dgergel@rhg.com\"}\n",
    "climo_obs_ds = climo_obs_fine.to_dataset(name='tmax')\n",
    "climo_obs_ds.attrs.update(climo_attrs)\n",
    "climo_obs_ds.to_netcdf(\"/home/jovyan/gmfd_test_climo.nc\")'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "climo_obs_fine = xr.open_dataset(\"/home/jovyan/gmfd_test_climo.nc\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Interpolate obs climo: fine -> coarse "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "distributed.utils_perf - WARNING - full garbage collections took 30% CPU time recently (threshold: 10%)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reuse existing file: /home/jovyan/obs_to_mod_bilinear_spatial_disagg.nc\n",
      "CPU times: user 32.8 ms, sys: 12.4 ms, total: 45.2 ms\n",
      "Wall time: 26.1 ms\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "distributed.utils_perf - WARNING - full garbage collections took 30% CPU time recently (threshold: 10%)\n",
      "distributed.utils_perf - WARNING - full garbage collections took 31% CPU time recently (threshold: 10%)\n"
     ]
    }
   ],
   "source": [
    "%%time \n",
    "obs_to_mod_weights = '/home/jovyan/obs_to_mod_bilinear_spatial_disagg.nc'\n",
    "regridder_obs_to_mod = xe.Regridder(tmax_obs.isel(time=0).rename({'latitude': 'lat', 'longitude': 'lon'}), \n",
    "                                    tmax_model.isel(time=0).rename({'latitude': 'lat', 'longitude': 'lon'}), \n",
    "                         'bilinear', filename=obs_to_mod_weights, reuse_weights=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 30.3 ms, sys: 2.96 ms, total: 33.3 ms\n",
      "Wall time: 26.1 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "climo_obs_coarse_lazy = xr.map_blocks(apply_weights, regridder_obs_to_mod, \n",
    "                                args=[climo_obs_fine.rename({'latitude': 'lat', 'longitude': 'lon'})])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "distributed.utils_perf - WARNING - full garbage collections took 30% CPU time recently (threshold: 10%)\n",
      "distributed.utils_perf - WARNING - full garbage collections took 30% CPU time recently (threshold: 10%)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 16.3 s, sys: 426 ms, total: 16.7 s\n",
      "Wall time: 16.4 s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "distributed.utils_perf - WARNING - full garbage collections took 31% CPU time recently (threshold: 10%)\n",
      "distributed.utils_perf - WARNING - full garbage collections took 31% CPU time recently (threshold: 10%)\n",
      "distributed.utils_perf - WARNING - full garbage collections took 31% CPU time recently (threshold: 10%)\n"
     ]
    }
   ],
   "source": [
    "%%time \n",
    "climo_obs_coarse = climo_obs_coarse_lazy.persist()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Compute scaling factor by subtracting for temperature, dividing for precip, the BC'ed model data at model-res from obs climo at model-res. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "distributed.utils_perf - WARNING - full garbage collections took 30% CPU time recently (threshold: 10%)\n",
      "distributed.utils_perf - WARNING - full garbage collections took 28% CPU time recently (threshold: 10%)\n"
     ]
    }
   ],
   "source": [
    "def _calculate_anomaly(ds, climo, var_name='temperature'):\n",
    "    # Necessary workaround to xarray's check with zero dimensions\n",
    "    # https://github.com/pydata/xarray/issues/3575\n",
    "    da = ds[var_name]\n",
    "    if sum(da.shape) == 0:\n",
    "        return da\n",
    "    groupby_type = ds.day_of_year\n",
    "    gb = da.groupby(groupby_type)\n",
    "    \n",
    "    return gb - climo\n",
    "\n",
    "def compute_scale_factor(spec):\n",
    "    '''\n",
    "    computes scale factor at the coarse level \n",
    "    '''\n",
    "    da_adj, da_obs_climo_coarse, var_name = spec\n",
    "    \n",
    "    return xr.map_blocks(_calculate_anomaly, da_adj, \n",
    "                         args=[da_obs_climo_coarse, var_name])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "chunks = {'time': 1500, 'latitude': len(tmax_model.latitude), \n",
    "                               'longitude': len(tmax_model.longitude)}\n",
    "\n",
    "tmax_model = tmax_model.chunk(chunks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "display(tmax_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''spec = (tmax_model, climo_obs_coarse)\n",
    "scale_factor_coarse = compute_scale_factor(spec)\n",
    "\n",
    "def compute_scale_factor(spec):\n",
    "    '''\n",
    "    computes scale factor at the coarse level \n",
    "    '''\n",
    "    da_adj, da_obs_climo_coarse, var_name = spec\n",
    "    \n",
    "    return xr.map_blocks(_calculate_anomaly, da_adj, \n",
    "                         args=[da_obs_climo_coarse, var_name])'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time \n",
    "scale_factor_coarse = xr.map_blocks(_calculate_anomaly, tmax_model, args=[climo_obs_coarse, 'tasmax'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scale_factor_coarse['time'] = tmax_model.time.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time \n",
    "sfc = scale_factor_coarse.persist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''%%time \n",
    "sfc = sfc.compute()'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# if starting from this point \n",
    "sfc = xr.open_dataset('/home/jovyan/sfc_test.nc')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Interpolate scaling factor: coarse (model grid) -> fine (obs grid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "mod_to_obs_weights = '/home/jovyan/mod_to_obs_bilinear_weights.nc'\n",
    "regridder_mod_to_obs = xe.Regridder(tmax_model.rename({'latitude': 'lat', 'longitude': 'lon'}), \n",
    "                                    tmax_obs.rename({'latitude': 'lat', 'longitude': 'lon'}), \n",
    "                         'bilinear', filename=mod_to_obs_weights, reuse_weights=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Functions for replicating xESMF functionality for running on workers since this is too large to run in notebook memory and xESMF is not setup to be used in conjunction with dask. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_xesmf_weights_coo_matrix(weights_file, size_in, size_out):\n",
    "    ds = xr.open_dataset(weights_file)\n",
    "    n_s = ds.dims['n_s']\n",
    "    col = ds['col'].values - 1\n",
    "    row = ds['row'].values - 1\n",
    "    S = ds['S'].values\n",
    "    A = coo_matrix((S, (row, col)), shape=[size_out, size_in]) \n",
    "    return A\n",
    "\n",
    "def apply_weights(spec):\n",
    "    weights, da, shape_in, shape_out, lats_out, lons_out = spec\n",
    "    indata = da.values\n",
    "    \n",
    "    shape_horiz = shape_in[-2:]\n",
    "    extra_shape = shape_in[0:-2]\n",
    "    \n",
    "    if len(shape_in) > 2:\n",
    "        indata_flat = indata.reshape(shape_in[0], shape_in[1]*shape_in[2])\n",
    "    else: \n",
    "        indata_flat = indata.reshape(-1, shape_in[0]*shape_in[1])\n",
    "    \n",
    "    outdata_flat = weights.dot(indata_flat.T).T\n",
    "    \n",
    "    outdata = outdata_flat.reshape(\n",
    "            [shape_out[0], shape_out[1]])\n",
    "        \n",
    "    if len(shape_in) > 2:\n",
    "        dims = {'time': da.time,'latitude': lats_out, 'longitude': lons_out}\n",
    "        coords = {'time': da.time, 'latitude': lats_out, 'longitude': lons_out}\n",
    "    else:\n",
    "        dims = {'latitude': lats_out, 'longitude': lons_out}\n",
    "        coords = {'latitude': lats_out, 'longitude': lons_out}\n",
    "    \n",
    "    \n",
    "    outdata_da = xr.DataArray(outdata, dims=dims, \n",
    "                              coords=coords)\n",
    "    \n",
    "    return outdata_da"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time \n",
    "# make scipy sparse weight matrix \n",
    "weights_coo_mat = read_xesmf_weights_coo_matrix(mod_to_obs_weights, size_model, size_obs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note: the method with `da.map_blocks` only works on a few years, beyond that we run out of memory. But I believe that this is the better way to do it - needs work. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "JOBS = [(weights_coo_mat, sfc['temperature'].isel(time=timestep).drop('time'), (180, 360), (720, 1440), \n",
    "         tmax_gmfd.latitude, tmax_gmfd.longitude) for timestep in sfc['temperature'].time]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sff_futures = client.map(apply_weights, JOBS[:20])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dd.progress(sff_futures)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sffs = client.gather(sff_futures)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sff = xr.concat(sffs, pd.Index(sfc.time.values, name='time'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Last step: add (or multiply for precip) the scaling factor to the obs-res daily climatology\n",
    "\n",
    "NOTE: this step has not been tested given that the memory for the above step needs to be further worked out, but it is essentially the inverse of the step above where we compute the scaling factor. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def apply_scale_factor(scale_factor_fine, obs_climo):\n",
    "    da = ds['temperature']\n",
    "    if 'dayofyear' in scale_factor_fine.dims:\n",
    "        scale_factor_fine.rename({'dayofyear': 'day_of_year'})\n",
    "        \n",
    "    sff_daily = scale_factor_fine.groupby(scale_factor_fine.day_of_year)\n",
    "    \n",
    "    return obs_climo + sff_daily\n",
    "\n",
    "def apply_scale_factor_wrapper(spec):\n",
    "    '''\n",
    "    applies scale factor to obs climatology\n",
    "    '''\n",
    "    scale_factor_fine, da_obs_climo_fine = spec\n",
    "    \n",
    "    return xr.map_blocks(apply_scale_factor, da_adj, args=[da_obs_climo_fine])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# chunk by year \n",
    "sff_chunk = sff.chunk({'time': 365, 'latitude': len(tmax_obs.latitude), 'longitude': len(tmax_obs.longitude)})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time \n",
    "spec = (sff_chunk, climo_obs_fine)\n",
    "model_ds = apply_scale_factor_wrapper(spec)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_ds = model_ds.persist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_downscaled = model_ds.compute()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Apply standardizing functions for final output and save (probably as zarr array)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
