{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import scipy\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import xarray as xr\n",
    "import collections\n",
    "import os \n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "import xesmf as xe\n",
    "\n",
    "import dask.distributed as dd\n",
    "import dask_kubernetes as dk\n",
    "import dask\n",
    "import dask.array as da\n",
    "import rhg_compute_tools.kubernetes as rhgk\n",
    "\n",
    "from scipy.sparse import coo_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'0.2.1'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "xe.__version__"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "NOTE: Can't replicate Jiawei's example notebook, https://github.com/JiaweiZhuang/sparse_dot/blob/master/scatter_large_weights.ipynb, without getting that pickling error that he claims to have solved in this issue: https://github.com/JiaweiZhuang/xESMF/issues/71. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### This notebook is a test of all the steps for Spatial Disaggregation to get a handle on the total CPU time it will take for this part of BCSD. \n",
    "\n",
    "Once-off steps: \n",
    "\n",
    "1. compute multi-decade daily climatologies of ERA-5 at obs-res and coarsen it to model-res (they, e.g. NASA-NEX, do not say how, we will do bilinear for consistency with later step)\n",
    "\n",
    "Per model/scenario/experiment steps:\n",
    "\n",
    "1. subtract (or divide for precip) BC’ed model data at model-res from obs climo at model resolution to calculate a “scaling factor” \n",
    "2. bilinearly interpolate “scaling factor” (using xESMF) from the model grid to the obs grid \n",
    "3. Apply scaling factor by adding (for temp) and multiplying (for precip) the “scaling factor” to the obs-res daily climatology \n",
    "\n",
    "NOTE: For the purpose of being conservative with timing, the \"coarsen obs climatology step to model-res\" is in the per model/scenario/experiment step, since we don't know for sure how/if CMIP6 models will be at exactly the same resolution. \n",
    "\n",
    "Currently this workflow is only built out for temperature, not precipitation. All steps are included, the last step (applying the interpolated scale factor to the obs-res daily climatology) has not yet been tested. All other parts of the workflow have been tested. The second to last step, the interpolation of the scaling factor from coarse to fine, is the most memory intensive, thus I have only tested for a subset of timesteps. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9794111aff774399b04487d77e1e1dc1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(HTML(value='<h2>KubeCluster</h2>'), HBox(children=(HTML(value='\\n<div>\\n  <style scoped>\\n    .…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "client, cluster = rhgk.get_standard_cluster()\n",
    "cluster"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _convert_lons(ds, lon_name='longitude'):\n",
    "    '''\n",
    "    converts longitudes from 0 to 360 to -180 to 180\n",
    "    '''\n",
    "    ds_conv_coords = ds.assign_coords(longitude=(((ds[lon_name] + 180) % 360) - 180))\n",
    "    ds_sort = ds_conv_coords.sel(**{lon_name: np.sort(ds_conv_coords[lon_name].values)})\n",
    "    return(ds_sort)\n",
    "\n",
    "def compute_climo(da):\n",
    "    climatology = da.groupby('time.dayofyear').mean('time')\n",
    "    \n",
    "    return climatology.compute()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load GMFD (obs) and Berkeley Earth (we are pretending BEST is a climate model that has already undergone bias correction here) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "tmax_obs = xr.open_mfdataset(os.path.join('/gcs/rhg-data/climate/source_data/GMFD/tmax', \n",
    "                                         'tmax_0p25_daily_199*'), concat_dim='time',\n",
    "                              parallel=True).squeeze(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# standardize longitudes \n",
    "tmax_obs = _convert_lons(tmax_obs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "tmax_model = xr.open_dataset(os.path.join('/gcs/rhg-data/climate/source_data/BEST/TMAX', \n",
    "                                         'Complete_TMAX_Daily_LatLong1_1990.nc'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Remove leap days from model and obs "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# remove leap days \n",
    "tmax_obs = tmax_obs.sel(time=~((tmax_obs.time.dt.month == 2) & (tmax_obs.time.dt.day == 29)))\n",
    "tmax_model = tmax_model.sel(time=~((tmax_model.month == 2) & (tmax_model.day == 29)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "size_model = len(tmax_model.latitude) * len(tmax_model.longitude)\n",
    "size_obs = len(tmax_obs.latitude) * len(tmax_obs.longitude)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Compute obs climatology (obs res)\n",
    "\n",
    "Note: in real workflow, this will be pre-computed (only need to do this once) and loaded. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "529fa9829fdf42358ee4065d5c0e9a91",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "tmax_obs_future = client.submit(compute_climo, tmax_obs['tmax'])\n",
    "dd.progress(tmax_obs_future)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "climo_obs_fine = client.gather(tmax_obs_future)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Interpolate obs climo: fine -> coarse "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reuse existing file: /home/jovyan/obs_to_mod_bilinear_weights.nc\n",
      "CPU times: user 24.5 ms, sys: 8.58 ms, total: 33.1 ms\n",
      "Wall time: 29.8 ms\n"
     ]
    }
   ],
   "source": [
    "%%time \n",
    "obs_to_mod_weights = '/home/jovyan/obs_to_mod_bilinear_weights.nc'\n",
    "regridder_obs_to_mod = xe.Regridder(tmax_obs.rename({'latitude': 'lat', 'longitude': 'lon'}), tmax_model.rename({'latitude': 'lat', 'longitude': 'lon'}), \n",
    "                         'bilinear', filename=obs_to_mod_weights, reuse_weights=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 721 ms, sys: 205 ms, total: 927 ms\n",
      "Wall time: 867 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "climo_obs_coarse = regridder_obs_to_mod(climo_obs_fine)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Compute scaling factor by subtracting for temperature, dividing for precip, the BC'ed model data at model-res from obs climo at model-res. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_anomaly(ds, climo):\n",
    "    # Necessary workaround to xarray's check with zero dimensions\n",
    "    # https://github.com/pydata/xarray/issues/3575\n",
    "    da = ds['temperature']\n",
    "    if sum(da.shape) == 0:\n",
    "        return da\n",
    "    groupby_type = ds.day_of_year\n",
    "    gb = da.groupby(groupby_type)\n",
    "    \n",
    "    return gb - climo\n",
    "\n",
    "def compute_scale_factor(spec):\n",
    "    '''\n",
    "    computes scale factor at the coarse level \n",
    "    '''\n",
    "    da_adj, da_obs_climo_coarse = spec\n",
    "    \n",
    "    return xr.map_blocks(calculate_anomaly, da_adj, args=[da_obs_climo_coarse.rename({'dayofyear': 'day_of_year'})])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# chunk by year \n",
    "tmax_model = tmax_model.chunk({'time': 365, 'latitude': len(tmax_model.latitude), 'longitude': len(tmax_model.longitude)})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 307 ms, sys: 2.6 ms, total: 310 ms\n",
      "Wall time: 279 ms\n"
     ]
    }
   ],
   "source": [
    "%%time \n",
    "spec = (tmax_model, climo_obs_coarse)\n",
    "scale_factor_coarse = compute_scale_factor(spec)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "scale_factor_coarse['time'] = tmax_model.time.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 24.5 ms, sys: 1.03 ms, total: 25.5 ms\n",
      "Wall time: 24.1 ms\n"
     ]
    }
   ],
   "source": [
    "%%time \n",
    "sfc = scale_factor_coarse.persist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'%%time \\nsfc = sfc.compute()'"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time \n",
    "sfc = sfc.compute()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "# if starting from this point \n",
    "sfc = xr.open_dataset('/home/jovyan/sfc_test.nc')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Interpolate scaling factor: coarse (model grid) -> fine (obs grid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reuse existing file: /home/jovyan/mod_to_obs_bilinear_weights.nc\n",
      "CPU times: user 69.9 ms, sys: 125 ms, total: 195 ms\n",
      "Wall time: 183 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "mod_to_obs_weights = '/home/jovyan/mod_to_obs_bilinear_weights.nc'\n",
    "regridder_mod_to_obs = xe.Regridder(tmax_model.rename({'latitude': 'lat', 'longitude': 'lon'}), \n",
    "                                    tmax_obs.rename({'latitude': 'lat', 'longitude': 'lon'}), \n",
    "                         'bilinear', filename=mod_to_obs_weights, reuse_weights=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Functions for replicating xESMF functionality for running on workers since this is too large to run in notebook memory and xESMF is not setup to be used in conjunction with dask. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_xesmf_weights_coo_matrix(weights_file, size_in, size_out):\n",
    "    ds = xr.open_dataset(weights_file)\n",
    "    n_s = ds.dims['n_s']\n",
    "    col = ds['col'].values - 1\n",
    "    row = ds['row'].values - 1\n",
    "    S = ds['S'].values\n",
    "    A = coo_matrix((S, (row, col)), shape=[size_out, size_in]) \n",
    "    return A\n",
    "\n",
    "def apply_weights(spec):\n",
    "    weights, da, shape_in, shape_out, lats_out, lons_out = spec\n",
    "    indata = da.values\n",
    "    \n",
    "    shape_horiz = shape_in[-2:]\n",
    "    extra_shape = shape_in[0:-2]\n",
    "    \n",
    "    if len(shape_in) > 2:\n",
    "        indata_flat = indata.reshape(shape_in[0], shape_in[1]*shape_in[2])\n",
    "    else: \n",
    "        indata_flat = indata.reshape(-1, shape_in[0]*shape_in[1])\n",
    "    \n",
    "    outdata_flat = weights.dot(indata_flat.T).T\n",
    "    \n",
    "    '''if len(shape_in) > 2:\n",
    "        outdata = outdata_flat.reshape(\n",
    "            [shape_in[0], shape_out[0], shape_out[1]])\n",
    "    else: \n",
    "        outdata = outdata_flat.reshape(\n",
    "            [*extra_shape, shape_out[0], shape_out[1]])'''\n",
    "    outdata = outdata_flat.reshape(\n",
    "            [shape_out[0], shape_out[1]])\n",
    "        \n",
    "    if len(shape_in) > 2:\n",
    "        dims = {'time': da.time,'latitude': lats_out, 'longitude': lons_out}\n",
    "        coords = {'time': da.time, 'latitude': lats_out, 'longitude': lons_out}\n",
    "    else:\n",
    "        dims = {'latitude': lats_out, 'longitude': lons_out}\n",
    "        coords = {'latitude': lats_out, 'longitude': lons_out}\n",
    "    \n",
    "    \n",
    "    outdata_da = xr.DataArray(outdata, dims=dims, \n",
    "                              coords=coords)\n",
    "    \n",
    "    return outdata_da"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 58.8 ms, sys: 25.1 ms, total: 83.9 ms\n",
      "Wall time: 79.5 ms\n"
     ]
    }
   ],
   "source": [
    "%%time \n",
    "# make scipy sparse weight matrix \n",
    "weights_coo_mat = read_xesmf_weights_coo_matrix(mod_to_obs_weights, size_model, size_obs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note: the method with `da.map_blocks` only works on a few years, beyond that we run out of memory. But I believe that this is the better way to do it - needs work. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''chunk_size = (1,180,360)\n",
    "client.rebalance()\n",
    "spec = (weights_coo_mat, sfc, chunk_size, (720, 1440), tmax_gmfd.latitude, tmax_gmfd.longitude)\n",
    "sff_lazy = da.map_blocks(apply_weights, spec, dtype=np.float64, chunks=chunk_size)\n",
    "sff_eager = client.compute(sff_lazy).result()'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "JOBS = [(weights_coo_mat, sfc['temperature'].isel(time=timestep).drop('time'), (180, 360), (720, 1440), \n",
    "         tmax_gmfd.latitude, tmax_gmfd.longitude) for timestep in sfc['temperature'].time]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sff_futures = client.map(apply_weights, JOBS[:20])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dd.progress(sff_futures)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sffs = client.gather(sff_futures)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sff = xr.concat(sffs, pd.Index(sfc.time.values, name='time'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Last step: add (or multiply for precip) the scaling factor to the obs-res daily climatology\n",
    "\n",
    "NOTE: this step has not been tested given that the memory for the above step needs to be further worked out, but it is essentially the inverse of the step above where we compute the scaling factor. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def apply_scale_factor(scale_factor_fine, obs_climo):\n",
    "    da = ds['temperature']\n",
    "    if 'dayofyear' in scale_factor_fine.dims:\n",
    "        scale_factor_fine.rename({'dayofyear': 'day_of_year'})\n",
    "        \n",
    "    sff_daily = scale_factor_fine.groupby(scale_factor_fine.day_of_year)\n",
    "    \n",
    "    return obs_climo + sff_daily\n",
    "\n",
    "def apply_scale_factor_wrapper(spec):\n",
    "    '''\n",
    "    applies scale factor to obs climatology\n",
    "    '''\n",
    "    scale_factor_fine, da_obs_climo_fine = spec\n",
    "    \n",
    "    return xr.map_blocks(apply_scale_factor, da_adj, args=[da_obs_climo_fine])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# chunk by year \n",
    "sff_chunk = sff.chunk({'time': 365, 'latitude': len(tmax_obs.latitude), 'longitude': len(tmax_obs.longitude)})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time \n",
    "spec = (sff_chunk, climo_obs_fine)\n",
    "model_ds = apply_scale_factor_wrapper(spec)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_ds = model_ds.persist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_downscaled = model_ds.compute()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Apply standardizing functions for final output and save (probably as zarr array)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
